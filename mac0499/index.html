<!DOCTYPE html>
<html>

<head>
  <meta charset="UTF-8">
  <title>Desenvolvimento e análise de uma biblioteca de autodiferenciação para ambientes distribuídos</title>
  <style>
    body {
      max-width: 1000px;
      margin: 40px auto;
      padding: 0 10px;
      font: 18px/1.5 -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, "Noto Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";
      color: #444
    }

    h1,
    h2,
    h3 {
      line-height: 1.2;
      text-align: left;
      margin: 0 0 20px;
    }

    @media (prefers-color-scheme: dark) {
      body {
        color: #c9d1d9;
        background: #0d1117
      }

      a:link {
        color: #58a6ff
      }

      a:visited {
        color: #8e96f0
      }
    }

    table {
      border: #0d1117;
      width: 100%;
    }

    th,
    td {
      text-align: left;
      padding: 8px;
      border-bottom: 1px solid #ddd;
    }

    th {
      background-color: #f2f2f2;
      color: #333;
    }

    tr:nth-child(even) {
      background-color: #f2f2f2;
    }

    tr:hover {
      background-color: #ddd;
    }
  </style>
</head>

<body>
  <h1>Desenvolvimento e análise de uma biblioteca de autodiferenciação para ambientes distribuídos</h1>

  <ul>
    <li>Orientador: Professor Daniel Macêdo Batista</li>
    <li>Aluno: Luã Nowacki Scavacini Santilli</li>
  </ul>

  <h2>Introdução</h2>
  <p>
    O trabalho proposto busca desenvolver uma biblioteca de
    autodiferenciação que permite que o treinamento de uma rede neural
    seja executado de forma distribuída. Ele envolveria o desenvolvimento
    da determinada biblioteca, incluindo a especificação de um protocolo
    de camada de aplicação para a comunicação entre os nós do ambiente
    distribuído, assim como uma análise comparativa dos resultados desta
    com outras bibliotecas do campo a partir do treinamento de uma rede
    neural com alguns datasets disponíveis publicamente.
  </p>

  <h2>Justificativa</h2>
  <p>
    O trabalho proposto é relevante porque aborda a otimização do treinamento de redes neurais em ambientes
    distribuídos, o que é uma demanda crescente no campo da Inteligência Artificial. Além disso, o desenvolvimento de
    uma biblioteca de autodiferenciação permitirá uma maior flexibilidade e customização dos modelos de redes neurais,
    possibilitando avanços significativos na área. A especificação de um protocolo de comunicação entre os nós do
    ambiente distribuído é crucial para garantir a eficiência e a escalabilidade do treinamento em larga escala. Por
    fim, a análise comparativa dos resultados com outras bibliotecas existentes é importante para avaliar a efetividade
    da abordagem proposta e fornecer informações valiosas para futuros desenvolvimentos na área.
  </p>

  <h2>Objetivos</h2>
  <ol>
    <li>Desenvolver uma biblioteca de operações em tensor</li>
    <li>Desenvolver uma biblioteca que implementa autodiferenciação sobre esses tensores</li>
    <li>Desenvolver uma arquitetura e protocolo de redes para realizar o treinamento de uma rede neural de forma
      distribuída</li>
    <li>Analisar os resultados do desenvolvimento com outras bibliotecas do campo.</li>
  </ol>

  <h2>Metodologia</h2>
  <ol>
    <li>Estudo de implementações de bibliotecas de autodiferenciação tais como tensorflow, pytorch e tinygrad.</li>
    <li>Estudo de bibliotecas que implementam a parte de inferência sobre modelos de forma optimizada: whisper.cpp,
      llmapa.cpp</li>
    <li>Estudo de algoritmos de aprendizado e arquitetura de redes que implementam aprendizado de forma distribuída como
      o _DistBelief_ do Google</li>
    <li>Desenvolvimento da biblioteca de operações de tensor</li>
    <li>Implementação de autodiferenciação sobre essa biblioteca</li>
    <li>Desenvolvimento de protocolos e arquiteturas que permitam o treinamento de forma distribuída</li>
    <li>Análise e comparação entre as arquiteturas, assim como possível continuação do desenvolvimento das arquiteturas
      baseado no conhecimento adquirido na análise.</li>
    <li>Treinamento de algum modelo de ML não trivial nesta arquitetura, comparando com resultados em outras
      arquiteturas/bibliotecas</li>
  </ol>

  <h2>Cronograma</h2>
  <table>
    <thead>
      <tr>
        <th>Mês</th>
        <th>Ponto de entrega intermediário</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Abril</td>
        <td>Estudo e definição das arquiteturas a serem desenvolvidas</td>
      </tr>
      <tr>
        <td>Maio</td>
        <td>Implementação da biblioteca de operação em tensores</td>
      </tr>
      <tr>
        <td>Junho</td>
        <td>Implementação da biblioteca de autodiferenciação</td>
      </tr>
      <tr>
        <td>Julho</td>
        <td>Implementação de protocolos e arquiteturas de redes 1</td>
      </tr>
      <tr>
        <td>Agosto</td>
        <td>Implementação de protocolos e arquiteturas de redes 2</td>
      </tr>
      <tr>
        <td>Setembro</td>
        <td>Análise das arquiteturas desenvolvidas</td>
      </tr>
      <tr>
        <td>Outubro</td>
        <td>Continuação do desenvolvimento baseado na análise</td>
      </tr>
      <tr>
        <td>Novembro</td>
        <td>Treinamento de um modelo não trivial na arquitetura desenvolvida</td>
      </tr>
  </table>
  <p>
    Este cronograma é apenas uma estimativa e pode sofrer alterações de acordo com o
    andamento do projeto. Além disso, pode haver a necessidade de incluir outras etapas
    não previstas inicialmente.
  </p>

  <p>
    Ao final do projeto, espera-se ter uma biblioteca de operações em tensor com autodiferenciação
    e uma arquitetura de rede distribuída para treinamento de modelos de aprendizado de máquina.
    A análise dos resultados permitirá comparar a eficácia da solução proposta com outras bibliotecas
    e arquiteturas disponíveis no mercado.
  </p>

  <h2>Desenvolvimento</h2>

  <div>
    <h3>
      Diferentes abordagens para treinamento dístribuido <a href="#ref-1"><sup>[1]</sup></a>
    </h3>

    <ul>
      <li>
        <h3><em>Model Parallel</em> vs <em>Data Parallel</em></h3>
        <p>
          Uma das principais distinções entre as estratégias de treinamento distribuído reside na distinção entre o
          paralelismo a nível do modelo empregado e o paralelismo a nível dos dados utilizados para calcular o modelo.
          Notando que abordagens híbridas são possíveis.
        </p>
        <p>
          No paralelismo a nível do modelo, este pode ser dividio em diferentes partições
          que podem então serem processadas em diferentes máquinas. Dessa forma, para realizar
          a inferência, é necessário que os sinais do modelo sejam transmitidos entre as máquinas.
        </p>
        <p>
          Já no paralelimso a nível de dados, o modelo é replicado em diferentes máquinas e cada
          uma delas recebe uma parte dos dados para realizar o treinamento. Dessa forma, não é necessário
          transmitir os sinais do modelo entre as máquinas, mas é necessário transmitir os dados e os valores
          dos pesos e gradientes necessários.
        </p>
        <p>
          A nível da abordagem de paralelimso de dados, por exemplo, pode-se definir um servidor de parametros
          e diferentes máquinas para computar os gradientes a nível de dados. Nesse caso, o servidor de parametros
          é responsável por receber os gradientes de cada máquina e atualizar os valores dos pesos do modelo.
          Esse tipo de abordagem pode acelerar o treinamento do modelo, pois ao aumentar o tamanho de uma
          <em>mini-batch</em>
          pode-se gerar um melhor estimador do gradiente, e por conseguinte, uma maior taxa de aprendizado. <a
            href="#ref-2"><sup>[2]</sup></a>
        </p>
      </li>
      <li>
        <h3>Otimização Centralizada vs Decentralizada</h3>
        <p>

        </p> 
      </li>
      <li>
        <h3>Sincrono vs Assíncrono</h3>
        <p>
          Outra distinção importante é se os nós da rede devem operar de forma síncrona
          ou não.
        </p>
      </li>

    </ul>


  </div>

  <h2>Referências</h2>
  <ol>
    <li id="ref-1">
      [1] Langer, Matthias, Zhen He, Wenny Rahayu, and Yanbo Xue. “Distributed Training of Deep Learning Models: A Taxonomic
      Perspective.” IEEE Transactions on Parallel and Distributed Systems 31, no. 12 (December 1, 2020): 2802–18.
      https://doi.org/10.1109/TPDS.2020.3003307.
    </li>
    <li id="ref-2">
      [2] McCandlish, Sam, Jared Kaplan, and Dario Amodei. “An Empirical Model of Large-Batch Training,” n.d.
    </li>
  </ol>




</body>